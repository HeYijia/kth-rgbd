\chapter{Introduction}

To navigate in an unknown environment, a mobile robot needs to build a map of the environment and locate itself in the map at the same time. The process adressing this dual problem is called SLAM, which stands for Simultaneous Localization and Mapping. In an outdoor environment, this can generally be solved by a GPS that generally allows to get an estimation of the position with a very good accuracy. However, when moving indoor or in places where the GPS data is not available, or not reliable enough, other solutions have to be found. 

Generally, the main problem raised with SLAM comes from the measurement errors, due to the sensors noise. Probablistic models are widely used to reduce these errors and provide satisfying estimations. While the SLAM process is generally based on data provided by sensors such as laser scanners, or odometry data, Visual SLAM focuses on the use of camera. 

\section{Context}

Currently, most of robotic mapping is performed using sensors that offers only a 2D cross section of the environment around them. The reason for this is ways of acquiring high quality 3D data were either very expensive or had hard constraints on the robot movements. However, recently there has been a great interest in processing data acquired using depth measuring sensors due to the availability of cheap and high performance RGB-D cameras. For instance, the Kinect Camera developed by Prime Sense and Microsoft has considerably changed the situation, providing a 3D camera at a very affordable price, with a particular interest for the research projects in robotics.

\section{Goals}

The goal of this work is to build a 3D map from RGB \& depth information provided by a camera. The hardware device used here is the Microsoft Kinect. The main objective of this work is to have an overview of the different methods and techniques in order to build a consistent map, so the problems can better be identified and then be individually handled more deeply. Even if Visual SLAM is intended to be used in real time, some of the processing may be done without focusing on the performance issues in this study. 

The Visual SLAM process can be described as estimating the poses of the camera in order to reconstruct the entire environment while the camera is moving. As the sensor noise lead to deviations in the estimations of each camera poses with respect to the the real motion, the goal is to build a 3D map which is close to the real environment as much as possible.

\section{Thesis outline}
The rest of the document is structured as follows:
\begin{description}
\item[Chapter 2] presents the Background and the underlying concepts that are most commonly used in this area.
\item[Chapter 3] presents the Method and describe the approach used in this work.
\item[Chapter 4] presents the Results with examples of maps obtained from different datasets.
\item[Chapter 5] presents the Future Works and the possible improvements.
\end{description}

\chapter{Background}

\section{Microsoft Kinect}

As mentioned in the introduction, the hardware used in this work is the Microsoft Kinect, a device developed by PrimeSense, initially for the Microsoft Xbox 360 and released in November 2010. It is composed by an RGB camera, 3D depth sensors, a multi-array microphone and a motorized tilt. In this work, only the RGB and depth sensors are used to provide the input data.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{figures/kinect_sensor}
\caption{Microsoft Kinect}
\end{figure}

Kinect characteristics
\begin{itemize}
\item The field of view is 57\textdegree horizontal 43\textdegree vertical, with a tilt range of $\pm$ 27\textdegree.
\item The RGB sensor is a regular camera that streams video with 8 bits for every color channel, giving a 24-bit color depth. Its color filter array is a Bayer filter mosaic. The color resolution is 640x480 pixels and a maximal frame rate of 30 Hz.
\item The depth sensing system is composed by an IR emitter and a standard CMOS Sensor. The IR light source projects structured light, which is then captured by the CMOS image sensor, and decoded to produce the depth image of the scene. It has an operation range between 0.7 and 6 meters, although best results are obtained in the range that goes from 1.2 to 3.5 meters. Its data output has 12-bit depth. The depth sensor resolution is 320x240 pixels with a rate of 30Hz.
\end{itemize}

The drivers used are those developed by OpenNI (Open Natural Interface). OpenNI is an organization composed by PrimeSense Willow Garage, Side-Kick, ASUS.


\section{Features}

An approach widely used in computer vision, especially in object recognition, is based on the detection of points of interests on object or surfaces. This is done through the extraction of features. In order to track these points of interests during a movement of the camera and/or the robot, a reliable feature detection has to be invariant to image location, scale and rotation. A few methods are briefly presented here:
\begin{description}
\item[SIFT] Scale Invariant Feature Transform
\item[SURF] Speeded Up Robust Feature
\item[NARF] Normal Aligned Radial Feature
\end{description}

\subsection{SIFT}
The method presented by David Lowe \cite{lowesift} is probably the most common in robotics and computer vision.
This is a method to detect distinctive, invariant image feature points, which easily can be matched between images to perform tasks such as object detection and recognition, or to compute geometrical transformations between images.

Several libraries exist. For this work is used the library developed by Rob Hess \cite{hesssift}.

\subsection{SURF}
SURF (Speeded Up Robust Feature) is a robust image detector \& descriptor \cite{surf}, that can be used in computer vision tasks like object recognition or 3D reconstruction. It is partly inspired by the SIFT descriptor. The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT. SURF is based on sums of approximated 2D Haar wavelet responses and makes an efficient use of integral images. (Wiki)

\subsection{NARF}
This method is presented in \cite{steder10irosws}. The NARF descriptors are part of the ROS Point Cloud Library.

\section{Matching}

Once the features have been computed, the question is to be able to track them during the movement of the camera. This is done by associating them between different frames. Here we only consider the matching for a couple of frames, the process can then be repeated on the whole sequence of frames.

\subsection{RANSAC}
The RANdom SAmple Consensus is an iterative method, where the main idea is to randomly select a minimal number of samples to determine a candidate transformation, evaluate it, and iterate with new samples by keeping the best transformation found. After running this for a fixed number of steps, the algorithm converges to a transformation with a lower error, but it does not necessarily find the best one, as all the possibilities have not been tested.

Define the number of iterations.
For each iteration:
Pickup k samples randomly and determine a model (hypothesis). Evaluate this model by computing the error for all the points of the dataset. If enough points are valid (number of inliers), then the hypothesis is valid. Keep the best hypothesis with respect to the the mean error (lowest is best).

\subsection{ICP}
Iterative Closest Point. As any gradient descent method, the ICP is applicable when we have in advance an relatively good starting point. Otherwise, it will be trapped into a local minimum. One possibility, as done by Intel Labs~\cite{IntelRGBD2010}, is to run RANSAC before, to determine the first guess for the ICP procedure.

\section{SLAM}

As described in the introduction, the SLAM problem can be defined as looking for the best estimation of the robot/camera poses, by reducing the uncertainty due to the noise affecting the measurements.

[Probabilistic model ???]

\subsection{Filtering vs Smoothing}

To solve the SLAM problem, the litterature presents different approaches can be classified either as filtering or smoothing. Filtering approaches model the problem as an on-line state estimation where the state of the system consists in the current robot position and the map. The estimate is augmented and refined by incorporating the new measurements as they become available.
The most common techniques are the Kalman filters (EKF) and the particle filters. To highlight their incremental nature, the filtering approaches are usually referred to as on-line SLAM methods.
The major drawback of these techniques is the computational cost. As the robot moves, the quantity of data to be handled becomes bigger.

Conversely, smoothing approaches estimate the full trajectory of the robot from the full set of measurements. These approaches address the so-called full SLAM problem, and they typically rely on least-square error minimization techniques.

Recently, the performance of the graph optimization has been dramatically improved. In this work, we will therefore study the results that can be obtained from a graph-based approach.

\subsection{Pose Graph}

The SLAM problem can be defined as a least squares optimization of an error function described by a graph. Such a graph is called a pose graph. In a general way, the problem can be formulated by a graph where the nodes (or vertices) represent a state variable describing the poses of the cameras, and the edges would represent the related constraints between the observations connecting a pair of nodes. Not only the graph representation gives a quite intuitive representation of the problem, but the graph model is also the base for the mathematical optimization, which purpose is to find the most-likely configuration of the nodes.

Here we present different approaches:
\begin{description}
\item[GraphSLAM] A probabilistic approach ~\cite{Thrun2005} : theoretical background
\item[HOG-Man] Hierarchical Optimizations on Manifolds for Online 2D and 3D mapping\cite{hogman2010}: this approach considers different levels. Solving the problem on lower levels affects only partially the upper levels. Well adapted for large maps.
\item[g2o] General Framework for Graph Optimization \cite{g2o2011}, by the same authors of HOG-Man\cite{hogman2010}. g2o is a framework which gives the possibility to redefine very precisely the error function to be minized and how the solver should perform it. Some classes are provided for the most common problems such as 2D/3D SLAM or Bundle Adjustement. The HOG-Man concepts are likely to be integrated in g2o. This is the method chosen to solve the graph problem in this work.
\end{description}

\subsection{Loop closure}
In order to minimize the error, the graph optimization relies on constraints between the nodes. One interesting event occurs when the robot moves back to a position already visited in the past. This is refered as the loop closure. Without making an assumption on the path followed by the robot, and simply by keeping the history of the past frames, it is possible to check if the current frame matches with some of the previous ones. If the current observation is close to a previous one, a transformation can be computed between these frames and a new constraint can be inserted from it. This can be repeated several times. With these new constraints, the cumulated error of the graph can then be considerably reduced. 

\chapter{Method}

The approach chosen in this work can be described in the following steps:
\begin{enumerate}
\item Feature extraction: SIFT extraction in 2D
\item Feature matching: kd-tree in 2D/3D
\item Transformation: RANSAC in 3D
\item Graph optimization: Based on the g2o framework
\item Loop Closures: Add some constraints in the graph
\item Scene reconstruction: Compute 3D point clouds from RGB+D data, project them accordingly to the estimations of the camera positions, and concatenate them to build the global map.
\end{enumerate}

\section{Features}
\subsection{Features Extraction}

The extraction of the SIFT features is done on the 2D RGB image. In order to keep only features that are interesting for the next steps of the processing, the features are removed if the depth data on this point is not available (occlusion) or if the depth data is outside a fixed range of depth distance.

Practically, there are many parameters to define how the SIFT features are extracted. Here the default parameters of the SIFT library are used.

\subsection{Features Matching}

Now that the features can be computed, it is possible to perform some feature matching between a couple of frames, to get the initial matches. This is done through a nearest neighbour search, using the euclidian distance of these features.  
The SIFT Library proposes an implementation based on kd-tree search. For each feature in the target frame, the 2 nearest neighbours are searched in the source frame. If these neighbours are close enough (with respect to a fixed threshold), then the result is considered to be valid. This first search is done on the 2D features.
After his, the depth information is used to keep the features that are close enough. A relative ratio is used.

Example of frame matching: TODO

\section{Transformation with RANSAC}

Done from the initial matches.
How to define the quality of transformation? 
Mean error, number of inliers, rate of inliers wrt initial matches.

\begin{figure}[h]
\centering
\includegraphics[height=0.5\textheight]{figures/bad_inliers1}
\caption{Bad inliers}
\end{figure}

\section{Graph optimization}
\subsection{Building the graph}

From each single transformations, an estimation of the poses of the camera can be computed. Each pose is then inserted into the graph by translating the camera matrix into a graph node. g2o provides the class SE3Quat which implies the transformation of the rotational part of the matrix into a quaternion.

\subsection{Solving the graph}

Various methods are available in g2o, for the optimization process and the solving problem. The method used here is Levenberg-Marquardt with a linear Cholmod solver. The optimization is done with a predefined number of steps. Once the graph has been optimized, the vertices have been updated and give the new estimations of the camera poses. The inverse operation is done to convert the information from the graph vertices to 4x4 matrices.

\subsection{Loop closures}

To detect a loop closure, it is necessary to compare recent frames with previous ones. The naive approach would be to compare the current frame with all the previous frames. Obviously, this is not realistic for reasons of performance, as the time necessary for the current frame would grow exponentially.
Without making any assumption of the past frames, the first idea would then be to define a window with a fixed size of n frames in the past. The most naive way is to select these frames randomly without any criterion. Another approach would be to select the past frames that are more likely to match with the current frame. One solution could be to give a higher priority to the oldest frames. This could be done by defining a probability density function that would result in a higher probability for the oldest frames. Another approach would also involve a knowledge of the context of the frames, in terms of features, or in terms of estimated position.

\subsection{Example of graph optimization}
In g2o there is a tool (g2oviewer) allowing the visualization of the graph.

Graph created.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/graph4_base}
\caption{Graph with 4 rooms, non optimized}
\end{figure}

Graph updated with the initial guess.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/graph4_initial_guess}
\caption{Graph with 4 rooms, initial guess)}
\end{figure}

Graph updated after 10 iterations.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/graph4_optimized}
\caption{Graph with 4 rooms optimized after 10 iterations}
\end{figure}

\clearpage

\section{3D scene reconstruction}

Once the camera poses are determined with a good belief, the reconstruction of the scene can be performed. From each couple of RGB and depth data, a 3D point cloud can be generated providing the colour information for each point, but they still have to be put together. One simple method is to concatenate the corresponding point clouds, each of them being transformed with the relative camera position. This can be done with standard functions provided by the PCL Library. For the visualization, the standard viewer is used.

For reasons of performance, it is necessary to control the memory storage by limitating to the size of the final point cloud. For this, each of the point clouds are first subsampled with a fixed ratio. Generally, only one output file is produced but this could still lead to a big file in case of a large map. A high rate of subsampling would give a map of low quality. By setting a threshold for the final point cloud, it is splitted in several subfiles each time the threshold is reached during the reconstruction of the global map. This allows to keep a low subsampling rate, and to visualize a portion of the global scene with a better definition.

Of course, this method is not optimal, as this leads to duplicate points, and does not take variances of illumination into account. Another solution would be to use some surfels \cite{IntelRGBD2010} but this would require some intensive processing and is out of the scope of this work.

\section{Software implementation}

The program built does the following tasks:
\begin{itemize}
\item Acquire RGB+depth data from the Kinect
\item Perform features extraction and matching on a sequence of frames and compute the relative transformations with RANSAC
\item Compute the positions of the camera poses and translate them into graph nodes and edges
\item Compute the loop closures and insert the corresponding edges into the graph
\item Optimize the graph and extract the updated camera poses
\item Reconstruct the global scene by generating PCD file
\end{itemize}

The number of dependencies has been limited as much as possible, using preferably open-source libraries.
Here are the main dependencies:
\begin{itemize}
\item OpenNI: to acquire the Kinect data
\item Eigen3: for the geometric support with transformations and matrices
\item SIFT Library by Rob Hess \cite{hesssift}: for the SIFT extraction and initial matching
\item OpenCV: open source library used to visualize the intermediate results with bitmaps (frame matching)
\item Point Cloud Libary, standalone distribution (cminpack, Flann, Eigen3, OpenNI): for the transformations and the export to Point Cloud Datafiles
\item Boost: support library especially for the filesystem access
\item ConfigFile: open source class to easily define configuration files
\end{itemize}

\chapter{Results}

The maps obtained from different datasets are shown. The first section presents the results with data from KTH, at different scales (1, 2 and 4 rooms). The next section shows the results obtained with data from other universities.

\section{CVAP (KTH)}

\subsection{1 room initial}
Map with 1 room without loop closure (no graph optimization). Note how the chair and a portion of the table appear to be doubled.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/room1_initial}
\caption{Map with 1 room (initial)}
\end{figure}

\subsection{1 room optimized}
Map with 1 room and 1 loop closures.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/room1_1}
\caption{Map with 1 room}
\end{figure}

\clearpage

\subsection{2 rooms}
Map with 2 rooms, optimized with 2 loop closures.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/room2_1}
\caption{Map with 2 rooms}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/room2_2}
\caption{Map with 2 rooms}
\end{figure}

\clearpage

\subsection{4 rooms}
Map with 4 rooms, optimized with 4 loop closures.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/room4_1}
\caption{Map with 4 rooms}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/room4_2}
\caption{Map with 4 rooms}
\end{figure}

\clearpage

\section{Other universities}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/bham}
\caption{Birmingham}
\end{figure}

\chapter{Future Works}

This work gave a good insight into the general problem SLAM, and more specifically about Visual SLAM with feature extraction and matching. Given the big picture, it is now possible to imagine some improvements of the current work, in terms of performance and quality of the results.

\section{Performance improvements}

Currently, most of the time is passed on the feature extraction. Remaining very close to the given method, the overall performance would significantly be improved by lowering the time spent for this step. This could be done through the use of new features:
\begin{itemize}
\item SURF: these features would probably give a result close to the SIFT features, with a lower computational time. However, this would imply to rewrite the matching code (the nearest neighbour based on the kd-tree was provided by the SIFT library).
\item SIFT GPU: a feature extraction optimized with hardware acceleration would give the same results in a very shorter time (about 1000s to 100ms for each feature extraction).
\end{itemize}

\section{Quality improvements}

The quality can be improved at different levels:
\begin{itemize}
\item Additional sensors: In this work, the only sensor used was the Kinect providing RGB+D data. One possibility would be to use others sensors and combine them to get a better result. This is particularly true when the transformations computed by the method presented in this work are less reliable. For example, when the quality of the transformations is not good, the odometry could be used assuming this is done for a short period of time.
\item ICP: the RANSAC method gives a transformation that is not necessarily the best. The ICP method could give a better result. The output of the RANSAC loop could be used as an initial guess for ICP.
\item Graph optimization: by changing the method used in g2o, deriving new classes and defining error function, working with different scales (hierarchical levels for bigger maps).
\item Loop closures: define new criterions for loop closures, by introducing some knowledge about the past frames (features or estimated position more likely to give a loop closure with the current frame).
\end{itemize}

\section{Other approach}

Microsoft Fusion: no feature (!) with real-time performance. Based on ICP, directly run on the point clouds from the depth data. However, the details of the method are not published yet (patents?). But the shown results seem to be very promising for the future.

