\chapter{Introduction}

To navigate in an unknown environment, a mobile robot needs to build a map of the environment and locate itself in the map at the same time. The process adressing this dual problem is called SLAM, which stands for Simultaneous Localization and Mapping. In an outdoor environment, this can generally be solved by a GPS that generally allows to get an estimation of the position with a very good accuracy. However, when moving indoor or in places where the GPS data is not available, or not reliable enough, other solutions have to be found. 

Generally, the main problem raised with SLAM comes from the measurement errors, due to the sensors noise. Probablistic models are widely used to reduce these errors and provide satisfying estimations. While the SLAM process is generally based on data provided by sensors such as laser scanners, or odometry data, Visual SLAM focuses on the use of camera. 

\section{Context}

Currently, most of robotic mapping is performed using sensors that offers only a 2D cross section of the environment around them. The reason for this is ways of acquiring high quality 3D data were either very expensive or had hard constraints on the robot movements. However, recently there has been a great interest in processing data acquired using depth measuring sensors due to the availability of cheap and high performance RGB-D cameras. For instance, the Kinect Camera developed by Prime Sense and Microsoft has considerably changed the situation, providing a 3D camera at a very affordable price, with a particular interest for the research projects in robotics.

\section{Goals}

The goal of this work is to build a 3D map from RGB \& depth information provided by a camera. The hardware device used here is the Microsoft Kinect. The main objective of this work is to have an overview of the different methods and techniques in order to build a consistent map, so the problems can better be identified and then be individually handled more deeply. Even if Visual SLAM is intended to be used in real time, some of the processing may be done without focusing on the performance issues in this study. 

The Visual SLAM process can be described as estimating the poses of the camera in order to reconstruct the entire environment while the camera is moving. As the sensor noise lead to deviations in the estimations of each camera poses with respect to the the real motion, the goal is to build a 3D map which is close to the real environment as much as possible.

\section{Thesis outline}

The rest of the document is structured as follows:
\begin{description}
\item[Chapter \ref{chap:background}] presents the Background and the underlying concepts that are most commonly used in this area.
\item[Chapter \ref{chap:features}] presents the Feature Matching and describe how a single 3D transformation can be computed  from a couple of frames by tracking some features.
\item[Chapter \ref{chap:graph}] presents the Graph Optimization and how the camera poses can be estimated through the use of a pose graph.
\item[Chapter \ref{chap:reconstruction}] presents the Scene Reconstruction with examples of maps obtained from different datasets.
\item[Chapter \ref{chap:conclusion}] presents the Conclusion and the Future Works with some suggestions of possible improvements.
\end{description}

\chapter{Background}
\label{chap:background}

\section{Microsoft Kinect}

As mentioned in the introduction, the hardware used in this work is the Microsoft Kinect, a device developed by PrimeSense, initially for the Microsoft Xbox 360 and released in November 2010. It is composed by an RGB camera, 3D depth sensors, a multi-array microphone and a motorized tilt. In this work, only the RGB and depth sensors are used to provide the input data.

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{pictures/kinect_sensor}
\caption{Microsoft Kinect}
\end{figure}

Kinect characteristics:
\begin{itemize}
\item The field of view is 57\textdegree horizontal 43\textdegree vertical, with a tilt range of $\pm$ 27\textdegree.
\item The RGB sensor is a regular camera that streams video with 8 bits for every color channel, giving a 24-bit color depth. Its color filter array is a Bayer filter mosaic. The color resolution is 640x480 pixels and a maximal frame rate of 30 Hz.
\item The depth sensing system is composed by an IR emitter projecting structured light, which is captured by the CMOS image sensor, and decoded to produce the depth image of the scene. It has an operation range between 0.7 and 6 meters, although best results are obtained in the range that goes from 1.2 to 3.5 meters. Its data output has 12-bit depth. The depth sensor resolution is 320x240 pixels with a rate of 30Hz.
\end{itemize}

The drivers used are those developed by OpenNI (Open Natural Interface). OpenNI is an organization composed by PrimeSense Willow Garage, Side-Kick, ASUS.

The main convenience with these drivers is that the calibration of the RGB sensor with respect to the IR sensor is ensured so the resulting RGB and depth data are correctly mapped with respect to a unique viewpoint.


\section{Features}

An approach widely used in computer vision, especially in object recognition, is based on the detection of points of interests on object or surfaces. This is done through the extraction of features. In order to track these points of interests during a movement of the camera and/or the robot, a reliable feature detection has to be invariant to image location, scale and rotation. A few methods are briefly presented here:
\begin{description}
\item[SIFT] Scale Invariant Feature Transform
\item[SURF] Speeded Up Robust Feature
\item[NARF] Normal Aligned Radial Feature
\end{description}

\subsection{SIFT}

The method presented by David Lowe~\cite{lowe_2004_sift} which is widely used in robotics and computer vision.
This is a method to detect distinctive, invariant image feature points, which easily can be matched between images to perform tasks such as object detection and recognition, or to compute geometrical transformations between images.

The main idea of the SIFT method is to define a cascade of operations following an increasing complexity, so that the most expensive operations are only performed to the most probable candidates.
\begin{enumerate}
\item The first step relies on a pyramid of difference-of-Gaussian in order to be invariant to scale and orientation.
\item From this, keypoints are determined with a more detailed model.
\item The image gradient directions is then used to assign one or more orientations to the keypoints.
\item The local image gradients are then transformed to be stable against distortion and changes in illumination.
\end{enumerate}
One of the main interest is to be able to perform fast searches on the features in order to identify candidate matching. Refer to the method described in~\cite{lowe_2004_sift} for further details.

Concerning the implementation, several libraries exist. For this work is used the library developed by Rob Hess~\cite{hess_sift}. Additionally to the feature extraction, this library also provides a kd-tree search function to perform the initial matching. There are also other functions such as a extendable function that can be used to compute geometrical transformations with RANSAC (method described in ~\ref{sub:ransac}) but the given functions are only working for 2D operations and will not be used.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{pictures/sift_matches_robhess}
\caption{SIFT features matched between the two images}
\end{figure}

\subsection{SURF}

SURF (Speeded Up Robust Feature) is a robust image detector \& descriptor~\cite{surf}, that can be used in computer vision tasks like object recognition or 3D reconstruction. It is partly inspired by the SIFT descriptor. The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT. SURF is based on sums of approximated 2D Haar wavelet responses and makes an efficient use of integral images. (Wiki)

\subsection{NARF}

This method is presented in~\cite{steder10irosws}. The NARF descriptors are part of the ROS Point Cloud Library~\cite{Rusu_ICRA2011_PCL}.

\begin{figure}[h!]
\centering
\includegraphics[width=0.6\textwidth]{pictures/narf1}
\caption{Example of NARF keypoints}
\end{figure}

\section{Matching}

Once the features have been computed, the question is to be able to track them during the movement of the camera. This is done by associating them between different frames. Here we only consider the matching for a couple of frames, the process can then be repeated on the whole sequence of frames. Associating several pairs of features is done with a given belief. Most of the algorithms work with different steps, first from a scarse level where the hypothesis is wide, which is then refined to eliminate the most obvious mismatches. The good matches are called the \emph{inliers}, while the matches that are shown to be errated are called the \emph{outliers}.

\subsection{RANSAC}
\label{sub:ransac}

The RANdom SAmple Consensus is an iterative method, where the main idea is to randomly select a minimal number of samples to determine a candidate transformation, evaluate it, and iterate with new samples by keeping the best transformation found. After running this for a fixed number of steps, the algorithm converges to a transformation with a lower error, but it does not necessarily find the best one, as all the possibilities have not been tested.

\begin{verbatim}
Define the number of iterations.
For each iteration:
Pickup k samples randomly and determine a model (hypothesis).
Evaluate this model by computing the error for all the points of the dataset.
If enough points are valid (number of inliers), then the hypothesis is valid.
Keep the best hypothesis with respect to the the mean error (lowest is best).
\end{verbatim}

\subsection{ICP}

The Iterative Closest Point is an algorithm presented by Zhang~\cite{zhang_92_icp}.
It iteratively revises the transformation (translation, rotation) needed to minimize the distance between the points of two raw scans.

Finds the rigid transformation minimizing the squared distance between each neighboring pairs $(p_{i}, p'_{i})$.

\[min \sum_{i}{||(Rp_{i}+t)-p'_{i}||}^2\]

As any gradient descent method, the ICP is applicable when we have in advance an relatively good starting point. Otherwise, it will be trapped into a local minimum. One possibility, as done by Intel Labs~\cite{Intel_RGBD_2010}, is to run RANSAC first to determine a good approximation of the rigid transformation, and use it as the first guess for the ICP procedure.

\section{SLAM}

As described in the introduction, the SLAM problem can be defined as looking for the best estimation of the robot/camera poses, by reducing the uncertainty due to the noise affecting the measurements. This leads to use the probabilistic approach widely described in the litterature such as~\cite{Thrun_2005}.

By denoting $x_t$ the state, $u_t$ a measurement, and $z_t$ the state at time t, the belief over the state variable $x_t$ is given by:

\[bel(x_t) = p(x_t | z_{1:t}, u_{1:t})\]

\subsection{Filtering vs Smoothing}

To solve the SLAM problem, the litterature presents different approaches can be classified either as filtering or smoothing. Filtering approaches model the problem as an on-line state estimation where the state of the system consists in the current robot position and the map. The estimate is augmented and refined by incorporating the new measurements as they become available.
The most common techniques are the Kalman filters (EKF) and the particle filters. To highlight their incremental nature, the filtering approaches are usually referred to as on-line SLAM methods.
The major drawback of these techniques is the computational cost. As the robot moves, the quantity of data to be handled becomes bigger.

Conversely, smoothing approaches estimate the full trajectory of the robot from the full set of measurements. These approaches address the so-called full SLAM problem, and they typically rely on least-square error minimization techniques.

Recently, the performance of the graph optimization has been dramatically improved. In this work, we will therefore study the results that can be obtained from a graph-based approach. But, in opposition to the online techniques, the pose graph is said to be offline or a "lazy" technique as the optimization processing triggered by specific constraints.

\subsection{Pose Graph}

The SLAM problem can be defined as a least squares optimization of an error function described by a graph. Such a graph is called a pose graph. In a general way, the problem can be formulated by a graph where the nodes (or vertices) represent a state variable describing the poses of the cameras, and the edges would represent the related constraints between the observations connecting a pair of nodes. Not only the graph representation gives a quite intuitive representation of the problem, but the graph model is also the base for the mathematical optimization, which purpose is to find the most-likely configuration of the nodes.

Here we present different approaches:
\begin{description}
\item[GraphSLAM] A probabilistic approach ~\cite{Thrun_2005} gives the theoretical background.
\item[TORO] a Tree-based netwORk Optimizer ~\cite{grisetti07rss}. A least square error minimization technique based on a network of relations is applied to find maximum likelihood (ML) maps. This is done by using a variant of gradient descent to minimize the error.
\item[HOG-Man] Hierarchical Optimizations on Manifolds for Online 2D and 3D mapping~\cite{hogman_2010}: this approach considers different levels. Solving the problem on lower levels affects only partially the upper levels. Well adapted for large maps.
\item[g2o] General Framework for Graph Optimization~\cite{g2o_2011}, by the same authors of HOG-Man~\cite{hogman_2010}. g2o is a framework which gives the possibility to redefine very precisely the error function to be minized and how the solver should perform it. Some classes are provided for the most common problems such as 2D/3D SLAM or Bundle Adjustement. The HOG-Man concepts are likely to be integrated in g2o. This is the method chosen to solve the graph problem in this work.
\end{description}

\subsection{Loop closure}

In order to minimize the error, the graph optimization relies on constraints between the nodes. One interesting event occurs when the robot moves back to a position already visited in the past. This is refered as the \emph{loop closure}. Without making an assumption on the path followed by the robot, and simply by keeping the history of the past frames, it is possible to check if the current frame matches with some of the previous ones. If the current observation is close to a previous one, a transformation can be computed between these frames and a new constraint can be inserted from it. This can be repeated several times. With these new constraints, the cumulated error of the graph can then be considerably reduced. 

\section{Summary}

This chapter gives an overview of some of the main concepts used for SLAM and more specifically Visual SLAM, defining the main lines of a possible approach to solve the problem. The features allow to track some points of interest and can be used to evaluate a unitary move from a couple of frames. By combining them together an estimation of the camera could be estimated. With the use of a pose graph, and after detecting loop closures, the graph could be optimized in order to reduce the global drift error. From the corrected camera poses, a map can then be built. 

While the performance is not the main issue of this work, it is still possible to make a distinction between the operations that can be performed \emph{online} (while the robot is moving) and the operations done \emph{offline} (after a given sequence of moves). While the SLAM adresses both Localization and Mapping, it seems clear that as long as the solution based on the graph implies the detection of loop closures to give a correct result, the dual problem of building the map and localizating the robot at the same time is realistically meant to be done \emph{offline}. This is one of the main constraint resulting of this approach. However, once the map is built, the single operation of localization could be done on the basis of the relevant features that have been used during the initial SLAM operation. So we can foresee that this kind of approach should allow the robot to build a map as a first step, only for this purpose. Once the map is built, the localization and further operations could be done but this mainly depends on the tasks the robot has to fulfill.

\chapter{Feature Matching}
\label{chap:features}

\section{Feature extraction}

The first question is the choice of the features. In the context of Visual SLAM, the goal is to track some keypoints precisely in order to compute the motion of the camera from the keypoint coordinates. For this, in addition to the scale and rotation invariance, one important condition that can be easily verified is to be stable if the camera does not move. 

Here we tested the SIFT features from Rob Hess Library~\cite{hess_sift} and the NARF descriptor~\cite{steder10irosws}. One important difference is that the SIFT feature is computed from the RGB data in 2D, while the NARF keypoints are computed from the 3D point cloud, involving both RGB and Depth data.
The NARF keypoints showed to be unstable, while the camera was not moving.

\begin{figure}[h!]
\centering$
\begin{array}{cc}
\includegraphics[width=0.5\textwidth]{pictures/narf1} &
\includegraphics[width=0.5\textwidth]{pictures/narf2}
\end{array}$
\caption{Example of NARF keypoints computed from two consecutive frames}
\end{figure}

This could be explained by the noise of the depth data. For this reason, we continued with the SIFT feature.

The extraction of the SIFT features is done on the 2D RGB image. In order to keep only features that are interesting for the next steps of the processing, the features are removed if the depth data on this point is not available (occlusion) or if the depth data is outside a fixed range of depth distance. Here a maximum range of 6 meters is used.

Practically, there are many parameters to define how the SIFT features are extracted. Here, the default parameters of the SIFT library~\cite{hess_sift} are used.

\section{Initial Matching}

Now that the features can be computed, it is possible to perform some feature matching between a couple of frames, to get the initial matches. This is done through a nearest neighbour search, using the euclidian distance of these features.  
The SIFT Library~\cite{hess_sift} proposes an implementation based on kd-tree search. For each feature in the target frame, the 2 nearest neighbours are searched in the source frame. If these neighbours are close enough (with respect to a fixed threshold), then the result is considered to be valid. This first search is done on the 2D features.
After his, the depth information is used to keep the features that are close enough. A relative ratio is used.

\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{pictures/sift_matching_init}
\caption{Initial matching of SIFT features}
\end{figure}

\section{Compute the 3D transformation}

The coordinates system is defined as follow:

[schema]
\(
\begin{array}{l}
x: depth, positive forward\\
y: height, positive upwards\\
z: width, positive to the right\\
\end{array}
\)

From the depth value, it is possible to compute the points coordinates in 3D, from the x and y screen coordinates:

\(
\begin{array}{l}
x = depth \\
y = (480/2 - y_{screen}) * depth / focal_length \\
z = (x_{screen} - 640/2) * depth / focal_length \\
\end{array}
\)

\paragraph{}
The transformation matrix then found from the initial matches with the RANSAC method, as presented in the background. Here there are two steps.

\begin{verbatim}
For each iteration:
pickup k samples from the initial matches
compute transformation
evaluate transformation -> set of inliers
store transformation if it is the best
pickup k samples from this set of inliers
compute new transformation from inliers
evaluate transformation inliers
store transformation if it is the best
\end{verbatim}

To compute a transformation (an hypothesis), only the k samples from the initial matches are used each time. The 3D points are converted to homogeneous coordinates and the transformation is given by solving the corresponding equation from the known constraints which are defined by the chosen points. Here, this is done through the class \emph{pcl::TransformationFromCorrespondences} from the PCL library~\cite{Rusu_ICRA2011_PCL}.

To evaluate a transformation, each sample taken from the initial matches (source) is then projected according to this transformation. A 3D vector is computed from the difference between the projected point and the real point taken from the matching point (destination). The error is the norm of this vector.

\begin{figure}[h!]
\centering$
\begin{array}{cc}
\includegraphics[width=0.4\textwidth]{pictures/sift_matching_init} &
\includegraphics[width=0.4\textwidth]{pictures/sift_matching_ransac}
\end{array}$
\caption{Matching of SIFT features - initial matching \& after a RANSAC loop. Note how the initial matches drawn in diagonal in the left figure are excluded and become outliers after the RANSAC loop.}
\end{figure}

How to characterize a transformation? There are at least 3 parameters:
\begin{itemize}
\item Mean error: the norm of the error vector. Lower is better.
\item Number of inliers: the absolute number of inliers. Higher is better.
\item Rate of inliers: the relative number of inliers with respect to the initial matches. Higher is better.
\end{itemize}

The main difficulty here is to find the best balance among these 3 values, by putting some thresholds. Setting to high constraints would lead to the impossibility to find a transformation satisfying all the criterions. This can be a problem when there are not enough features available. Setting too low constraints helps to find a transformation even when there are less features or the measurements are noisy, but the resulting transformation will be less precise. The main risk here is to put so loose constraints that the inliers can be wrong.

If the criterions are too permissive, this could result in an incorrect assocations.

\begin{figure}[h!]
\centering$
\includegraphics[width=0.4\textwidth]{pictures/bad_inliers1}$
\caption{Example of bad inliers - the matching are totally incorrect}
\end{figure}

\clearpage

How to measure the quality of a transformation?

A good transformation could be the one who is valid for most of the given points. But this definition is not enough, as considering only the number of inliers is not necessarily the best choice. Another criterion could be the spread of the inliers. If the inliers are all group together, they don't give much added value with respect to the global transformation of the scene. Many outliers are excluded but a better transformation may be found englobing more distant points. This could be measured by computing the mean of the inliers and from this the standard deviation, or more simply the variance of the inliers.

Let N be the number of inliers and $p_i$ be the i-th inlier vector. We can then compute the mean vector $\mu$ and the variance $\sigma^2$ with their standard definitions:
\[
\mu = \frac{1}{N} \sum_{i=1}^N{p_i}
\]
\[
\sigma^2 = \frac{1}{N} \sum_{i=1}^N{(p_i - \mu)^2}
\]

\begin{figure}[h!]
\centering$
\begin{array}{cccc}
\includegraphics[width=0.3\textwidth]{pictures/bad_transform1} &
\includegraphics[width=0.3\textwidth]{pictures/bad_transform2} &
\includegraphics[width=0.3\textwidth]{pictures/bad_transform3}
\end{array}$
\caption{Sequence showing different distributions of the inliers. Note how the inliers are grouped in the picture 2.}
\end{figure}

The corresponding values can be used to measure and detect this kind of situations.

\begin{tabular}{ccll}
inliers/matches & ratio & $\sigma^2(2D)$ & $\sigma^2(3D)$\\
\hline
93/114 &	81\% &	0.345409 &	1.2232\\
34/119 &	28\% &	0.0203726 &	0.0519737\\
91/135 &	67\% &	0.476902 &	1.42809\\
%90/140 &	64\% &	0.682466 &	1.56629\\
\end{tabular}

\chapter{Pose graph}
\label{chap:graph}

In the previous chapter, we saw how to determine a rigid transformation between a couple of frames. From each single transformation, an estimation of the poses of the camera can be computed. Now we can combine them to follow a sequence of frames. Each pose is then inserted into the graph by translating the camera matrix into a graph node. From the loop closures, new constraints can be inserted in the graph. The graph can then be optimized to globally reduce the error, and the poses updated according to the new vertices given by the graph after optimization.

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{figures/graph1}
\caption{Overview of the graph optimization procedure}
\end{figure}

\section{Estimating the poses}

Knowing the initial pose, the first step is to determine an estimation of any pose after a succession of transformations.

Considering a finite sequence of frames $[frame_0 ; frame_N]$, let~$P_k$ be the pose at rank $k \in [0;N]$, defined by a 4x4 matrix with homogeneous coordinates. For~$i>0$, we have the transformation~$T_{i_{-1},i}$ that binds the position~$P_{i-1}$ to the position~$P_i$. If~$P_0$ determines the initial position, we can then compute the position~$P_i$  by combining all the transformations like:

\begin{equation}
P_k = \prod_{i=k}^1{T_{i_{-1},i}} \: P_0
\label{eqn:pose_estimation}
\end{equation}

Note that, as the matrix product is not commutative, it is essential to follow the correct order when multiplying the matrices. For example, for the pose~$P_3$ we would have:

\[
P_3 = T_{2,3} \: T_{1,2} \: T_{0,1} \: P_0
\]

As an arbitrary choice, we can define the initial position to be at the origin of the coordinate system, ie the identity matrix~$I_4$.

\[
P_0 = I_4 = \left[ \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \end{array} \right] 
\]

\section{Building the graph}

The equation \ref{eqn:pose_estimation} gives an estimation of the pose that can be inserted into the g2o graph.
g2o provides the class SE3Quat which implies the transformation of the rotational part of the matrix into a quaternion.

\section{Solving the graph}

Various methods are available in g2o, for the optimization process and the solving problem. The method used here is Levenberg-Marquardt with a linear Cholmod solver. The optimization is done with a predefined number of steps. Once the graph has been optimized, the vertices have been updated and give the new estimations of the camera poses. The inverse operation is done to convert the information from the graph vertices to 4x4 matrices.

\section{Loop closures}

To detect a loop closure, it is necessary to compare recent frames with previous ones. The naive approach would be to compare the current frame with all the previous frames. Generally, this is not realistic for reasons of performance, as the time necessary for the current frame would grow exponentially, especially if the matching verification is time consuming, especially if the features of the past frames have to be recomputed each time. However, a preliminar check exclusively done on the RGB data, for example with color histograms, could be used to discard most of the negative candidates. Then, a more accurate verification with features could be done on the remaining candidates.

Without making any assumption of the past frames, the first idea would then be to define a window with a fixed size of n frames in the past. The most naive way is to select these frames randomly without any criterion.

Another approach would be to select the past frames that are more likely to match with the current frame. One solution could be to give a higher priority to the oldest frames. This could be done by defining a probability density function that would result in a higher probability for the oldest frames. Another approach would also involve a knowledge of the context of the frames, in terms of features, or in terms of estimated position.

\section{Example of graph optimization}
In g2o there is a tool (g2oviewer) allowing the visualization of the graph.

\begin{figure}[h!]
\centering$
\begin{array}{cc}
\includegraphics[width=0.4\textwidth]{pictures/graph4_base} &
\includegraphics[width=0.4\textwidth]{pictures/graph4_initial_guess}
\end{array}$
\caption{Graph with 4 rooms: non optimized - initial guess}
\end{figure}
\begin{figure}[h]
\centering$
\begin{array}{cc}
\includegraphics[width=0.4\textwidth]{pictures/graph4_initial_guess} &
\includegraphics[width=0.4\textwidth]{pictures/graph4_optimized}
\end{array}$
\caption{Graph with 4 rooms: initial guess - after 10 iterations}
\end{figure}

\clearpage

\chapter{Scene reconstruction}
\label{chap:reconstruction}

Once the camera poses are determined with a good belief, the reconstruction of the scene can be performed. From each couple of RGB and depth data, a 3D point cloud can be generated providing the colour information for each point, but they still have to be put together. This process is called the point cloud \emph{registration}. Once the relative camera positions are known, one simple method is to transform each point cloud by reprojecting all its points according to the corresponding camera position, and concatenate together all the transformed point clouds. This transformation can be done with standard functions provided by the PCL Library~\cite{Rusu_ICRA2011_PCL}. For the visualization, the standard viewer is used.

For reasons of performance, it is necessary to control the memory storage by limitating to the size of the final point cloud. Theorically, each point cloud can be composed of 640*480 = 307.200 points. In reality, this is less, as the depth information is not available for each point. A standard point cloud is still composed of about 200.000 points. Therefore, a colored cloud point occupies around 1.5MB in memory (24 bits for the positions x,y,z + 24 bits for the colors R,G,B + PADDING?! TOCHECK). For a scope of visualization, it is reasonable to reduce the amount of information. Each point cloud is first subsampled very basically by removing randomly points with a given probability (according to a fixed ratio).

Generally, only one output file is produced but this could still lead to a big file in case of a large map. In this case, the total number of points can still be relatively important and a too high rate of subsampling would lead to a map of low quality. By setting a threshold for the size of the final point cloud file, it can be splitted in several subfiles each time the threshold is reached during the reconstruction of the global map. The result is then divided in different files. This allows to keep a low subsampling rate, and still to visualize a portion of the global scene with a good definition.

This method remains simple, but the major issues are that it leads to duplicate points, and it does not take variances of illumination into account. The point cloud is not necessarily the best representation of the scene. Another solution would be to use some \emph{surfels} as done at Intel~Labs~\cite{Intel_RGBD_2010}, but this would require some intensive processing and is out of the scope of this work.

The next sections present the maps obtained from different datasets. The first section presents the results with data from KTH, at different scales (1 room, 2 and 4 rooms with corridor). The next section shows the results obtained with data from other universities.

\section{Map CVAP (KTH)}

\subsection{Map with 1 room, without graph optimization}

This map is built from a sequence containing 1 room without loop closure (no graph optimization). 

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{pictures/room1_initial}
\caption{Map with 1 room, without optimization (initial). Note how the corner of the table and the chair appears to be doubled.}
\end{figure}

\subsection{Map with 1 room, graph optimized with 1 loop closure}
This map is built from a sequence containing 1 room with 1 loop closure, which is triggered after having gone through the room and back to a position close to the initial position. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{pictures/room1_1}
\caption{Map with 1 room, optimized with 1 loop closure. Note how the corner of the table and the chair are now correctly "merged" compared to the previous figure.}
\end{figure}

\clearpage

\subsection{Map with 2 rooms and corridor}
Map with 2 rooms, optimized with 2 loop closures.

\begin{figure}[h!]
\centering$
\begin{array}{c}
\includegraphics[width=0.8\textwidth]{pictures/room2_1}\\
\includegraphics[width=0.8\textwidth]{pictures/room2_2}
\end{array}$
\caption{Map with 2 rooms}
\end{figure}

\clearpage

\subsection{Map with 4 rooms and corridor}
Map with 4 rooms, optimized with 4 loop closures.

\begin{figure}[h]
\centering$
\begin{array}{c}
\includegraphics[width=0.8\textwidth]{pictures/room4_1}\\
\includegraphics[width=0.8\textwidth]{pictures/room4_2}
\end{array}$
\caption{Map with 4 rooms and corridor}
\end{figure}

\clearpage

\section{Other universities}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{pictures/bham}
\caption{Birmingham}
\end{figure}

\chapter{Conclusions and Future Works}
\label{chap:conclusion}

This work gave a good insight into the general problem SLAM, and more specifically about Visual SLAM with feature extraction and matching.

The approach can be described in the following steps:
\begin{enumerate}
\item Feature extraction: SIFT extraction in 2D
\item Feature matching: kd-tree in 2D/3D
\item Transformation: RANSAC in 3D
\item Graph optimization: Based on the g2o framework
\item Loop Closures: Add some constraints in the graph
\item Scene reconstruction: Compute 3D point clouds from RGB+D data, project them accordingly to the estimations of the camera positions, and concatenate them to build the global map.
\end{enumerate}

Given the big picture, it is now possible to imagine some improvements of the current work, in terms of performance and quality of the results.

\section{Performance improvements}

Currently, most of the time is passed on the feature extraction. Remaining very close to the given method, the overall performance would significantly be improved by lowering the time spent for this step. This could be done through the use of new features:
\begin{itemize}
\item SURF: these features would probably give a result close to the SIFT features, with a lower computational time, such as done in the RGBD-6D-SLAM project~\cite{engelhard11euron-workshop}. However, this would imply to rewrite the matching code (the nearest neighbour based on the kd-tree was provided by the SIFT library~\cite{hess_sift}).
\item SIFT GPU: a feature extraction optimized with hardware acceleration would give the same results in a very shorter time (about 1000s to 100ms for each feature extraction).
\end{itemize}

\section{Quality improvements}

The quality can be improved at different levels:
\begin{itemize}
\item Additional sensors: In this work, the only sensor used was the Kinect providing RGB+D data. One possibility would be to use others sensors and combine them to get a better result. This is particularly true when the transformations computed by the method presented in this work are less reliable. For example, when the quality of the transformations is not good, the odometry could be used assuming this is done for a short period of time.
\item ICP: the RANSAC method gives a transformation that is not necessarily the best. The ICP method could give a better result. The output of the RANSAC loop could be used as an initial guess for ICP.
\item Graph optimization: by changing the method used in g2o, deriving new classes and defining error function, working with different scales (hierarchical levels for bigger maps).
\item Loop closures: define new criterions for loop closures, by introducing some knowledge about the past frames (features or estimated position more likely to give a loop closure with the current frame).
\end{itemize}

\section{Other approach}

Microsoft Fusion: no feature (!) with real-time performance. Based on ICP, directly run on the point clouds from the depth data. However, the details of the method are not published yet (patents?). But the shown results seem to be very promising for the future.

\begin{appendices}
\chapter{Annex - Software implementation}

The program built does the following tasks:
\begin{itemize}
\item Acquire RGB+depth data from the Kinect
\item Perform features extraction and matching on a sequence of frames and compute the relative transformations with RANSAC
\item Compute the positions of the camera poses and translate them into graph nodes and edges
\item Compute the loop closures and insert the corresponding edges into the graph
\item Optimize the graph and extract the updated camera poses
\item Reconstruct the global scene by generating PCD file
\end{itemize}

The number of dependencies has been limited as much as possible, using preferably open-source libraries.
Here are the main dependencies:
\begin{itemize}
\item OpenNI: to acquire the Kinect data
\item Eigen3: for the geometric support with transformations and matrices
\item SIFT Library by Rob Hess~\cite{hess_sift}: for the SIFT extraction and initial matching
\item OpenCV: open source library used to visualize the intermediate results with bitmaps (frame matching)
\item Point Cloud Libary~\cite{Rusu_ICRA2011_PCL}, standalone distribution (cminpack, Flann, Eigen3, OpenNI): for the transformations and the export to Point Cloud Datafiles (*.pcd)
\item Boost: support library especially for the filesystem access
\item ConfigFile: open source class to easily define configuration files
\end{itemize}

\end{appendices}

