For a mobile robot exploring an unknown static environment, localizing itself and building a map at the same time is a chicken-or-egg problem, known as Simultaneous Localization And Mapping (SLAM).

When a GPS receiver cannot be used, such as in indoor environments, the measurements are generally provided by laser rangefinders and stereo cameras, but they are expensive and standard laser rangefinders offer only 2D cross sections. However, recently there has been a great interest in processing data acquired using depth measuring sensors due to the availability of cheap and performant RGB-D cameras. For instance, the Kinect developed by Prime Sense and Microsoft has considerably changed the situation, providing a 3D camera at a very affordable price.

In this study, we will see how a 3D map based on a graphical model can be built by tracking visual features like SIFT/SURF, computing geometric transformations with RANSAC, and applying non-linear optimization techniques to estimate the trajectory. This can be done from a sequence of video frames combined with the depth information, using exclusively the Kinect, so the field of applications can be wider than robotics.