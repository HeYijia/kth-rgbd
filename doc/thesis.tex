%%
%% This is file `thesis.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% kthesis.dtx  (with options: `ex1')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from thesis.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file kthesis.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[latin1]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{nada-ex}
\usepackage{graphicx}

\title{Building a 3D map from RGB-D sensors}
\author{Virgile H\"{o}gman}
\date{September 2011}
\blurb{Master's Thesis at CVAP\\Supervisor: Alper Aydemir\\Examiner: Stefan Carlsson}
\trita{TRITA xxx yyyy-nn}

\begin{document}
\frontmatter
\maketitle
\input{kth-abs}
\clearpage
\selectlanguage{swedish}
%\begin{abstract}
%  Denna fil ger ett avhandlingsskelett.
%  Mer information om \LaTeX-mallen finns i
%  dokumentationen till paketet.
%\end{abstract}
\selectlanguage{english}
\clearpage
\tableofcontents
\mainmatter

\chapter{Introduction}

Currently, most of robotic mapping is performed using sensors that offers only a 2D cross section of the environment around them. The reason for this is ways of acquiring high quality 3D data were either very expensive or had hard constraints on the robot movements. However, recently there has been a great interest in processing data acquired using depth measuring sensors due to the availability of cheap and high performance RGB-D cameras. 

\section{Context}

SLAM (Simultaneous Localization and Mapping) adresses the dual problem of building a map and locate the robot at the same time. While this process is generally based on odometry data provided by different sensors, Visual SLAM focuses on the use of camera, for instance 3D. The process is then to estimate the poses of the camera in order to reconstruct the entire environment while the camera is moving.

The main problem comes from the measurement errors that leads to deviations in the estimations of each camera poses with respect to the the real motion. Probablistic models are widely used to reduce these errors and provide satisfying estimations.

\section{Goals}

The goal of this work is to build a 3D map from RGB \& depth information provided by a camera. The hardware device used here is the Microsoft Kinect. While Visual SLAM is intended to be used in real time, some of the processing may be done without focusing on the performance issues. The main objective of this work is to have an overview of the different methods and techniques in order to build a consistent map, so the problems can better be identified and then be handled individually.

\section{Thesis outline}
The rest of the document is structured as follows:
\begin{description}
\item[Chapter 2] presents the Background and the underlying concepts.
\item[Chapter 3] presents the Method.
\item[Chapter 4] presents the Results obtained with different datasets.
\item[Chapter 5] presents the Future Works.
\end{description}

\chapter{Background}

\section{Microsoft Kinect}

The hardware used in this work is the Microsoft Kinect, a device developed by PrimeSense, initially for the Microsoft Xbox 360 and released in November 2010. It is composed by an RGB camera, 3D depth sensors, a multi-array microphone and a motorized tilt. In this work, only the RGB and depth sensors are used to provide the input data.

Kinect characteristics
The RGB sensor is a regular camera that streams video with 8 bits for every color channel, giving a 24-bit depth. Its color filter array is a Bayer filter mosaic. The compound system has VGA resolution (640x480 pixels) and a maximal frame rate of 30 Hz. The field of view is 57\textdegree horizontal 43\textdegree vertical, with a tilt range of $\pm$ 27\textdegree.

The depth sensing system is composed by an IR emitter and a standard CMOS Sensor. The IR light source projects structured light, which is then captured by the CMOS image sensor, and decoded to produce the depth image of the scene. It has an operation range between 0.7 and 6 meters, although best results are obtained in the range that goes from 1.2 to 3.5 meters. Its data output has 12-bit depth.

The drivers used are those developed by OpenNI (Open Natural Interface). OpenNI is an organization composed by PrimeSense Willow Garage, Side-Kick, ASUS.


\section{Features}

An approach widely used in computer vision, especially in object recognition, is based on the detection of points of interests on object or surfaces. This is done through the extraction of features. In order to track these points of interests during a movement of the camera and/or the robot, a reliable feature detection has to be invariant to image location, scale and rotation. A few methods are briefly presented here:
\begin{description}
\item[SIFT] Scale Invariant Feature Transform
\item[SURF] Speeded Up Robust Feature
\item[NARF] Normal Aligned Radial Feature
\end{description}

\subsection{SIFT}
David Lowe.
This is a method to detect distinctive, invariant image feature points, which easily can be matched between images to perform tasks such as object detection and recognition, or to compute geometrical transformations between images.

Several libraries exist. For this work is used the library developed by Rob Hess.

\subsection{SURF}
Source Wiki.
SURF (Speeded Up Robust Feature) is a robust image detector \& descriptor, first presented by Herbert Bay et al. in 2006, that can be used in computer vision tasks like object recognition or 3D reconstruction. It is partly inspired by the SIFT descriptor. The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT. SURF is based on sums of approximated 2D Haar wavelet responses and makes an efficient use of integral images.

\subsection{NARF}
This method is presented in [ref] and is part of ROS Point Cloud Library.

\section{Matching}

\subsection{RANSAC}
RANdom SAmple Consensus. Iterative method.

Define the number of iterations.
For each iteration:
Pickup k samples randomly and determine a model (hypothesis). Evaluate this model by computing the error for all the points of the dataset. If enough points are valid (number of inliers), then the hypothesis is valid. Keep the best hypothesis with respect to the the mean error (lowest is best).

\subsection{ICP}
Iterative Closest Point. As any gradient descent method, the ICP is applicable when we have in advance an relatively good starting point. Otherwise, it will be trapped into a local minimum. One possibility, as done by Intel Labs~\cite{IntelRGBD2010}, is to run RANSAC before, to determine the first guess for the ICP procedure.

\section{SLAM}

As described in the introduction, the SLAM problem can be defined as looking for the best estimation of the robot/camera poses, by reducing the uncertainty due to the noise affecting the measurements.

\subsection{Filtering vs Smoothing}

To solve the SLAM problem, the litterature presents different approaches can be classified either as filtering or smoothing. Filtering approaches model the problem as an on-line state estimation where the state of the system consists in the current robot position and the map. The estimate is augmented and refined by incorporating the new measurements as they become available.
The most common techniques are the Kalman filters (EKF) and the particle filters. To highlight their incremental nature, the filtering approaches are usually referred to as on-line SLAM methods.
The major drawback of these techniques is the computational cost. As the robot moves, the quantity of data to be handled becomes bigger.

Conversely, smoothing approaches estimate the full trajectory of the robot from the full set of measurements. These approaches address the so-called full SLAM problem, and they typically rely on least-square error minimization techniques.

Recently, the performance of the graph optimization has been dramatically improved. In this work, we will therefore study the results that can be obtained from a graph-based approach.

\subsection{Pose Graph}

The SLAM problem can be defined as a least squares optimization of an error function described by a graph. Such a graph is called a pose graph. In a general way, the problem can be formulated by a graph where the nodes (or vertices) represent a state variable describing the poses of the cameras, and the edges would represent the related constraints between the observations connecting a pair of nodes. Not only the graph representation gives a quite intuitive representation of the problem, but the graph model is also the base for the mathematical optimization, which purpose is to find the most-likely configuration of the nodes.

\begin{description}
\item[HOG-Man] Hierarchical
\item[g2o] General Framework for Graph Optimization
\item[GraphSLAM] See ~\cite{Thrun2005}
\end{description}

\chapter{Method}

The approach chosen in this work can be described in the following steps:
\begin{description}
\item[Feature extraction] SIFT extraction in 2D
\item[Feature matching] kd-tree in 2D/3D
\item[Transformation] RANSAC in 3D
\item[Graph optimization] Based on the g2o framework
\item[Scene reconstruction] Concatenate the point clouds based on the estimations of the camera positions
\end{description}

\section{Features}
\subsection{Features Extraction}

The extraction of the SIFT features is done on the 2D RGB image. In order to keep only features that are interesting for the next steps of the processing, the features are removed if the depth data on this point is not available (occlusion) or if the depth data is outside a fixed range of depth distance.

Practically, there are many parameters to define how the SIFT features are extracted. Here the default parameters of the SIFT library are used.

\subsection{Features Matching}

Now that the features can be computed, it is possible to perform some feature matching between a couple of frames, to get the initial matches. This is done through a nearest neighbour search, using the euclidian distance of these features.  
The SIFT Library proposes an implementation based on kd-tree search. For each feature in the target frame, the 2 nearest neighbours are searched in the source frame. If these neighbours are close enough (with respect to a fixed threshold), then the result is considered to be valid. This first search is done on the 2D features.
After his, the depth information is used to keep the features that are close enough. A relative ratio is used.

Example of frame matching: 

\section{Transformation with RANSAC}

Done from the initial matches.
How to define the quality of transformation? 
Mean error, number of inliers, rate of inliers wrt initial matches.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/bad_inliers1}
\caption{Bad inliers}
\end{figure}

\section{Graph optimization}
\subsection{Building the graph}

From each single transformations, an estimation of the poses of the camera can be computed. Each pose is then inserted into the graph by translating the camera matrix into a graph node. g2o provides the class SE3Quat which implies the transformation of the rotational part of the matrix into a quaternion.

\subsection{Solving the graph}

Various methods are available in g2o, for the optimization process and the solving problem. The method used here is Levenberg-Marquardt with a linear Cholmod solver. The optimization is done with a predefined number of steps. Once the graph has been optimized, the vertices have been updated and give the new estimations of the camera poses. The inverse operation is done to convert the information from the graph vertices to 4x4 matrices.

\subsection{Loop closures}

Define the criterion for loop closure.

\section{3D scene reconstruction}

Once the camera poses are determined with a good belief, the reconstruction of the scene can be performed. From each couple of RGB and depth data, a 3D point cloud can be generated providing the colour information for each point, but they still have to be put together. One simple method is to concatenate the corresponding point clouds, each of them being transformed with the relative camera position. This can be done with standard functions provided by the PCL Library. For the visualization, the standard viewer is used.

For reasons of performance, it is necessary to control the memory storage by limitating to the size of the final point cloud. For this, each of the point clouds are first subsampled with a fixed ratio. Generally, only one output file is produced but this could still lead to a big file in case of a large map. A high rate of subsampling would give a map of low quality. By setting a threshold for the final point cloud, it is splitted in several subfiles each time the threshold is reached during the reconstruction of the global map. This allows to keep a low subsampling rate, and to visualize a portion of the global scene with a better definition.

Of course, this method is not optimal, as this leads to duplicate points, and does not take variances of illumination into account. Another solution would be to use some surfels \cite{IntelRGBD2010} but this would require some intensive processing and is out of the scope of this work.

\chapter{Results}

The maps obtained from different datasets are shown. The first section presents the results with data from KTH, at different scales (1, 2 and 4 rooms). The next section shows the results obtained with data from other universities.

\section{CVAP (KTH)}

\subsection{1 room}
Map with 1 room and 1 loop closures.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/room1}
\caption{Map with 1 room}
\end{figure}

\subsection{2 rooms}
Map with 2 rooms and n loop closures.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/room2}
\caption{Map with 1 room}
\end{figure}

\subsection{4 rooms}

\section{Other universities}

\chapter{Future Works}

This work gave a good insight into the general problem SLAM and more specifically about Visual SLAM with feature extraction and matching. Seeing the big picture, it is now possible to imagine some improvements of the current work.

\section{Performance improvements}

SURF: the SURF features would probably give a result close to the SIFT features, with a faster response. However this would imply to rewrite the matching code (the nearest neighbour based on the kd-tree was provided by the SIFT library).

SIFT GPU: a feature extraction optimized with hardware acceleration would give the same results in a very shorter time (about 1000s to 100ms for each feature extraction).

\section{Quality improvements}

- ICP
- graph optimization
- combine sensors
- define new criterions for loop closures.

\section{Other approach}

Microsoft Fusion: no feature with real-time performance. Based on ICP, directly run on the point clouds from the depth data. Details of the method not published yet (patents?).

\bibliographystyle{plainnat}  
\bibliography{references}

\end{document}
\endinput
%%
%% End of file `thesis.tex'.



